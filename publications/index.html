<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Forrest Davis</title> <meta name="author" content="Forrest Davis"> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar."> <meta name="keywords" content="computational linguistics, natural language processing, cross-linguistic, computational psycholinguistics, AI, interpretability, neural models of language"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%B2&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://forrestdavis.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Forrest </span>Davis</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/presentations/">presentations</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="zhang-2023-illusions" class="col-sm-8"> <div class="title">Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics</div> <div class="author"> <a href="https://sites.google.com/view/yhzhang" rel="external nofollow noopener" target="_blank">Yuhan Zhang</a>, <a href="http://tedlab.mit.edu/" rel="external nofollow noopener" target="_blank">Edward Gibson</a>, and <em>Forrest Davis</em> </div> <div class="periodical"> <em>In Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.01386" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Zhang-2023-Illusions-CoNLL.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/forrestdavis/LanguageIllusions" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Despite neural language models qualitatively capturing many human linguistic behaviors, recent work has demonstrated that they underestimate the true processing costs of ungrammatical structures. We extend these more fine-grained comparisons between humans and models by investigating the interaction between Principle B and coreference processing. While humans use Principle B to block certain structural positions from affecting their incremental processing, we find that GPT-based language models are influenced by ungrammatical positions. We conclude by relating the mismatch between neural models and humans to properties of training data and suggest that certain aspects of human processing behavior do not directly follow from linguistic data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang-2023-illusions</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Yuhan and Gibson, Edward and Davis, Forrest}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="davisLimitationsDataMismatches2022" class="col-sm-8"> <div class="title">On the Limitations of Data: Mismatches between Neural Models of Language and Humans</div> <div class="author"> <em>Forrest Davis</em> </div> <div class="periodical"> <em>Cornell University</em>, Dec 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ecommons.cornell.edu/items/142691cf-3f18-4e78-a405-a1c0b8b25738" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/DavisDissertation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/forrestdavis/Dissertation/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The majority of work at the intersection of computational linguistics and natural language processing aims to show, process by process, that human linguistic behavior (and knowledge) is reducible to a simple learning objective (e.g., predicting the next word) applied to unstructured linguistic data (e.g., written data). This dissertation uses three test cases to show concrete instances where current reductionist approaches fall short of human linguistic knowledge. In the first case study, implicit causality, competition among multiple linguistic processes is shown to obscure human-like behavior in models. This challenges existing methodologies that rely on the investigation of individual linguistic processes in isolation and points to a mismatch between human linguistic systems and those built solely on the basis of linguistic data. In the second case study, ambiguous relative clause attachment, models of Spanish and English are compared to show that, while models appear to mimic humans in English, they fail to do so in Spanish. The failure of computational models of Spanish follows from a mismatch between data produced by speakers and speakers’ interpretation preferences, and it is argued that this reflects fundamental limitations of text data. In the third case study, Principle B and incremental processing, it is demonstrated that, while humans use hard constraints to restrict their online processing of pronouns, computational models do not. The inability of models to process language incrementally like humans indicates a mismatch between linguistic data and the human parser. This dissertation argues that data are not sufficient to instruct models about fundamental aspects of human language. Ultimately, in using techniques from psycholinguistics and careful cross-linguistic comparison, it is argued that neural models can reveal specific areas of linguistic knowledge where data is not enough, suggesting in turn what the human mind itself must contribute.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">davisLimitationsDataMismatches2022</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the {{Limitations}} of {{Data}}: {{Mismatches}} between {{Neural Models}} of {{Language}} and {{Humans}}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{On the {{Limitations}} of {{Data}}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Davis, Forrest}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{Cornell University}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Ambiguous relative clause attachment,Computational linguistics,Implicit causality,Natural language processing,Neural models of language,Principle B}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="davis-2022-incremental" class="col-sm-8"> <div class="title">Incremental Processing of Principle B: Mismatches Between Neural Models and Humans</div> <div class="author"> <em>Forrest Davis</em> </div> <div class="periodical"> <em>In Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL)</em>, Dec 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2022.conll-1.11/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2022-Davis-PrincipleB-CoNLL.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/forrestdavis/PrincipleB" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.18653/v1/2022.conll-1.11"></span> <span class="__dimensions_badge_embed__" data-doi="10.18653/v1/2022.conll-1.11" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Despite neural language models qualitatively capturing many human linguistic behaviors, recent work has demonstrated that they underestimate the true processing costs of ungrammatical structures. We extend these more fine-grained comparisons between humans and models by investigating the interaction between Principle B and coreference processing. While humans use Principle B to block certain structural positions from affecting their incremental processing, we find that GPT-based language models are influenced by ungrammatical positions. We conclude by relating the mismatch between neural models and humans to properties of training data and suggest that certain aspects of human processing behavior do not directly follow from linguistic data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">davis-2022-incremental</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Incremental Processing of Principle {B}: Mismatches Between Neural Models and Humans}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Davis, Forrest}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Abu Dhabi, United Arab Emirates (Hybrid)}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.conll-1.11}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2022.conll-1.11}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{144--156}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="davis-van-schijndel-2021-uncovering" class="col-sm-8"> <div class="title">Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning</div> <div class="author"> <em>Forrest Davis</em>, and <a href="https://vansky.github.io/" rel="external nofollow noopener" target="_blank">Marten van Schijndel</a> </div> <div class="periodical"> <em>In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2021.acl-long.93" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2021-Davis-UncoveringConstraints-ACL.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/forrestdavis/ImplicitCausality" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/Davis-ACL2021-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.18653/v1/2021.acl-long.93"></span> <span class="__dimensions_badge_embed__" data-doi="10.18653/v1/2021.acl-long.93" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>A growing body of literature has focused on detailing the linguistic knowledge embedded in large, pretrained language models. Existing work has shown that non-linguistic biases in models can drive model behavior away from linguistic generalizations. We hypothesized that competing linguistic processes within a language, rather than just non-linguistic model biases, could obscure underlying linguistic knowledge. We tested this claim by exploring a single phenomenon in four languages: English, Chinese, Spanish, and Italian. While human behavior has been found to be similar across languages, we find cross-linguistic variation in model behavior. We show that competing processes in a language act as constraints on model behavior and demonstrate that targeted fine-tuning can re-weight the learned constraints, uncovering otherwise dormant linguistic knowledge in models. Our results suggest that models need to learn both the linguistic constraints in a language and their relative ranking, with mismatches in either producing non-human-like behavior.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">davis-van-schijndel-2021-uncovering</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Davis, Forrest and {van Schijndel}, Marten}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.acl-long.93}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2021.acl-long.93}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1159--1171}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="davis-altmann-2021-event" class="col-sm-8"> <div class="title">Finding Event Structure in Time: What Recurrent Neural Networks can tell us about Event Structure in Mind</div> <div class="author"> <em>Forrest Davis</em>, and <a href="http://www.psycholinguistics.com/gerry_altmann/index.html" rel="external nofollow noopener" target="_blank">Gerry Altmann</a> </div> <div class="periodical"> <em>Cognition</em>, Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Davis_Altmann_2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/forrestdavis/ExperimentNorming" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.1016/j.cognition.2021.104651"></span> <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.1016/j.cognition.2021.104651" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Under a theory of event representations that defines events as dynamic changes in objects across both time and space, as in the proposal of Intersecting Object Histories (Altmann &amp; Ekves, 2019), the encoding of changes in state is a fundamental first step in building richer representations of events. In other words, there is an inherent dynamic that is captured by our knowledge of events. In the present study, we evaluated the degree to which this dynamic was inferable from just the linguistic signal, without access to visual, sensory, and embodied experience, using recurrent neural networks (RNNs). Recent literature exploring RNNs has largely focused on syntactic and semantic knowledge. We extend this domain of investigation to representations of events within RNNs. In three studies, we find preliminary evidence that RNNs capture, in their internal representations, the extent to which objects change states; for example, that chopping an onion changes the onion by more than just peeling the onion. Moreover, the temporal relationship between state changes is encoded to some extent. We found RNNs are sensitive to how chopping an onion and then weighing it, or first weighing it, entails the onion that is being weighed being in a different state depending on the adverb. Our final study explored what factors influence the propagation of these rudimentary event representations forward into subsequent sentences. We conclude that while there is much still to be learned about the abilities of RNNs (especially in respect of the extent to which they encode objects as specific tokens), we still do not know what are the equivalent representational dynamics in humans. That is, we take the perspective that the exploration of computational models points us to important questions about the nature of the human mind.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">davis-altmann-2021-event</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Finding Event Structure in Time: What Recurrent Neural Networks can tell us about Event Structure in Mind}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Cognition}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{213}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{104651}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0010-0277}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.cognition.2021.104651}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0010027721000706}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Davis, Forrest and Altmann, Gerry}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Recurrent neural networks, Event representation, Discourse dependencies}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="davis-van-schijndel-2020-discourse" class="col-sm-8"> <div class="title">Discourse structure interacts with reference but not syntax in neural language models</div> <div class="author"> <em>Forrest Davis</em>, and <a href="https://vansky.github.io/" rel="external nofollow noopener" target="_blank">Marten van Schijndel</a> </div> <div class="periodical"> <em>In Proceedings of the 24th Conference on Computational Natural Language Learning</em>, Nov 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2020.conll-1.32/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/Davis-2020-ImplicitCausality-CoNLL.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/forrestdavis/ImplicitCausality" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.18653/v1/2020.conll-1.32"></span> <span class="__dimensions_badge_embed__" data-doi="10.18653/v1/2020.conll-1.32" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Language models (LMs) trained on large quantities of text have been claimed to acquire abstract linguistic representations. Our work tests the robustness of these abstractions by focusing on the ability of LMs to learn interactions between different linguistic representations. In particular, we utilized stimuli from psycholinguistic studies showing that humans can condition reference (i.e. coreference resolution) and syntactic processing on the same discourse structure (implicit causality). We compared both transformer and long short-term memory LMs to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information. Our results further suggest that LM behavior can contradict not only learned representations of discourse but also syntactic agreement, pointing to shortcomings of standard language modeling.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">davis-van-schijndel-2020-discourse</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Discourse structure interacts with reference but not syntax in neural language models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Davis, Forrest and {van Schijndel}, Marten}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Fern{\'a}ndez, Raquel and Linzen, Tal}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 24th Conference on Computational Natural Language Learning}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.conll-1.32}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2020.conll-1.32}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{396--407}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="davis-van-schijndel-2020-recurrent" class="col-sm-8"> <div class="title">Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment</div> <div class="author"> <em>Forrest Davis</em>, and <a href="https://vansky.github.io/" rel="external nofollow noopener" target="_blank">Marten van Schijndel</a> </div> <div class="periodical"> <em>In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, Jul 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2020.acl-main.179" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/Davis-2020-Attachment-ACL.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/forrestdavis/AmbiAttach" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.18653/v1/2020.acl-main.179"></span> <span class="__dimensions_badge_embed__" data-doi="10.18653/v1/2020.acl-main.179" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>A standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions (i.e. is a grammatical sentence more probable than an ungrammatical sentence). Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations, where stark grammaticality differences are absent. We compare model performance in English and Spanish to show that non-linguistic biases in RNN LMs advantageously overlap with syntactic structure in English but not Spanish. Thus, English models may appear to acquire human-like syntactic preferences, while models trained on Spanish fail to acquire comparable human-like preferences. We conclude by relating these results to broader concerns about the relationship between comprehension (i.e. typical language model use cases) and production (which generates the training data for language models), suggesting that necessary linguistic biases are not present in the training signal at all.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">davis-van-schijndel-2020-recurrent</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Recurrent Neural Network Language Models Always Learn {E}nglish-Like Relative Clause Attachment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Davis, Forrest and {van Schijndel}, Marten}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.acl-main.179}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2020.acl-main.179}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1979--1990}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="davisInteractionContextRecurrent2020" class="col-sm-8"> <div class="title">Interaction with Context During Recurrent Neural Network Sentence Processing</div> <div class="author"> <em>Forrest Davis</em>, and <a href="https://vansky.github.io/" rel="external nofollow noopener" target="_blank">Marten van Schijndel</a> </div> <div class="periodical"> <em>In Proceedings of the 42nd Annual Conference of the Cognitive Science Society</em>, May 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://cognitivesciencesociety.org/cogsci20/papers/0672/index.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/Davis-2020-Interaction-CogSci.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/forrestdavis/GardenPath" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.31234/osf.io/8r65d"></span> <span class="__dimensions_badge_embed__" data-doi="10.31234/osf.io/8r65d" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Syntactic ambiguities in isolated sentences can lead to increased difficulty in incremental sentence processing, a phenomenon known as a garden-path effect. This difficulty, however, can be alleviated for humans when they are presented with supporting discourse contexts. We tested whether recurrent neural network (RNN) language models (LMs) could learn linguistic representations that are similarly influenced by discourse context. RNN LMs have been claimed to learn a variety of syntactic constructions. However, recent work has suggested that pragmatically conditioned syntactic phenomena are not acquired by RNNs. In comparing model behavior to human behavior, we show that our models can, in fact, learn pragmatic constraints that alleviate garden-path effects given the correct training and testing conditions. This suggests that some aspects of linguistically relevant pragmatic knowledge can be learned from distributional information alone.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">davisInteractionContextRecurrent2020</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Interaction with {{Context During Recurrent Neural Network Sentence Processing}}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 42nd {{Annual Conference}} of the {{Cognitive Science Society}}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Davis, Forrest and {van Schijndel}, Marten}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2744--2750}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.31234/osf.io/8r65d}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="friedman-etal-2017-linguistically" class="col-sm-8"> <div class="title">Linguistically Rich Vector Representations of Supertags for TAG Parsing</div> <div class="author"> Dan Friedman, Jungo Kasai, R. Thomas McCoy, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Robert Frank, Forrest Davis, Owen Rambow' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms</em>, Sep 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/W17-6213" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/Davis-2017-TAG.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">friedman-etal-2017-linguistically</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Linguistically Rich Vector Representations of Supertags for {TAG} Parsing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Friedman, Dan and Kasai, Jungo and McCoy, R. Thomas and Frank, Robert and Davis, Forrest and Rambow, Owen}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Kuhlmann, Marco and Scheffler, Tatjana}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Ume{\aa}, Sweden}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/W17-6213}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{122--131}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Forrest Davis. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: November 11, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>